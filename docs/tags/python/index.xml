<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Rmar&#39;s Guide</title>
    <link>https://dektoud.github.io/blog/tags/python/</link>
    <description>Recent content in Python on Rmar&#39;s Guide</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Ryan Martin</copyright>
    <lastBuildDate>Mon, 12 Mar 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/blog/tags/python/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Calling Fortran DLL&#39;s from Python and Julia</title>
      <link>https://dektoud.github.io/blog/post/fortran_dlls/</link>
      <pubDate>Mon, 12 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dektoud.github.io/blog/post/fortran_dlls/</guid>
      <description>

&lt;p&gt;The standard Python library contains a package called &lt;code&gt;ctypes&lt;/code&gt; which provides interfaces to foreign functions that are found in shared libraries. With the Intel compiler, a Fortran function or subroutine can be exported to dll by ensuring that the &lt;code&gt;!DEC$&lt;/code&gt; is present to define the exported symbol:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;! Note: this only works with the Intel Compiler
subroutine sqr(val)
    !DEC$ ATTRIBUTES DLLEXPORT, ALIAS:&#39;sqr&#39; :: sqr
    integer, intent(inout) :: val
    val = val ** 2
end subroutine sqr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the above subroutine is contained in the &lt;code&gt;example.f90&lt;/code&gt; file, it is compiled with :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifort /dll example.f90
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which produces the shared library &lt;code&gt;example.dll&lt;/code&gt;. Inspecting this object with dumpbin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ dumpbin /exports example.dll
1    0 00001000 sqr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The function can be imported to Python with the &lt;code&gt;ctypes&lt;/code&gt; module and called with the following Python code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ctypes as ct

fortlib = ct.CDLL(&#39;example.dll&#39;)
val = 100
val = ct.pointer(ct.c_int(val)) # setup the pointer to the correct data structure
_ = fortlib.sqr(val)            # call the function
print(val[0])                   # index the memory address setup above
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is immediately clear that compiling the shared library in this way is easier (when compared to F2PY), and calling the Fortran from Python is much difficult. The advantages are that this is a C-independent method to build and link the Fortran extension, and the same dll called from Python here could also be called from other languages (e.g. Julia).&lt;/p&gt;

&lt;h1 id=&#34;maintaining-gnu-intel-compatibility&#34;&gt;Maintaining GNU - Intel compatibility&lt;/h1&gt;

&lt;p&gt;The method to call specific functions should be compiler independent. A method to maintain a compatible Python calling sequence for the dll&amp;rsquo;s is required since one set of Python code should be able to call the Fortran function regardless of how it was compiled. This can be done by using the &lt;code&gt;iso_c_binding&lt;/code&gt; module:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;subroutine sqr2(val) BIND(C, NAME=&#39;sqr2&#39;)
    use iso_c_binding
    !DEC$ ATTRIBUTES DLLEXPORT :: sqr2
    integer, intent(inout) :: val
    val = val ** 2
end subroutine sqr2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;iso_c_binding&lt;/code&gt; in this case overwrites the name mangling for each compiler (gfortran and ifort), and ensures the call sequence from Python is constant. In this case the &lt;code&gt;!DEC$&lt;/code&gt; statement is ignored by gfortran and the function is available in the exported dll. Notably, the &lt;code&gt;ALIAS&lt;/code&gt; is dropped from the &lt;code&gt;!DEC$&lt;/code&gt; export for Intel Fortran since the name in the binding overwrites this setting.&lt;/p&gt;

&lt;h1 id=&#34;more-complicated-data-types&#34;&gt;More Complicated Data Types&lt;/h1&gt;

&lt;p&gt;The main concern for calling Fortran functions in the dll&amp;rsquo;s from Python is how to setup the data structures. Consider the following function which take a 2-dimensional array of integers and computes some arbitrary value from the inputs by replacing the values in the array:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module example
    use iso_c_binding
    implicit none
contains
    subroutine sqr_2d_arr(nd, val) BIND(C, NAME=&#39;sqr_2d_arr&#39;)
        !DEC$ ATTRIBUTES DLLEXPORT :: sqr_2d_arr
        integer, intent(in) :: nd
        integer, intent(inout) :: val(nd, nd)
        integer :: i, j
        do j = 1, nd
        do i = 1, nd
            val(i, j) = (val(i, j) + val(j, i)) ** 2
        enddo
        enddo
    end subroutine sqr_2d_arr
end module example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The module is compiled with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifort /dll example.f90
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gfortran -shared example.f90 -o example.dll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and can be called from Python with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ctypes as ct
import numpy as np

# import the dll
fortlib = ct.CDLL(&#39;example.dll&#39;)

# setup the data
N = 10
nd = ct.pointer( ct.c_int(N) )          # setup the pointer
pyarr = np.arange(0, N, dtype=int) * 5  # setup the N-long
for i in range(1, N):                   # concatenate columns until it is N x N
    pyarr = np.c_[pyarr, np.arange(0, N, dtype=int) * 5]

# call the function by passing the ctypes pointer using the numpy function:
_ = fortlib.sqr_2d_arr(nd, np.ctypeslib.as_ctypes(pyarr))

print(pyarr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;important-considerations&#34;&gt;Important Considerations&lt;/h1&gt;

&lt;p&gt;In the above Python code some things are notable:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All data is passed by pointer reference to the Fortran dll, the data is modified in place if the variable is &lt;code&gt;intent(inout)&lt;/code&gt;, or replaced if the variable is &lt;code&gt;intent(out)&lt;/code&gt;. All memory is controlled on the Python side of things&lt;/li&gt;
&lt;li&gt;The memory must be allocated in some form on the Python side using &lt;code&gt;np.zeros(size, dtype=type)&lt;/code&gt; (or similar) even if the variable is &lt;code&gt;intent(out)&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;The types of all data initialized on the Python size must match those being called in the Fortran module&lt;/strong&gt;
a. This is very important. For example, use &lt;code&gt;int&lt;/code&gt; for &lt;code&gt;integer&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt; for &lt;code&gt;real*8&lt;/code&gt;, etc. Python &lt;code&gt;float&lt;/code&gt; is double precision by default. This may not be a correct list, and is definitely not exhaustive, but it has worked with the limited testing that has been done with this method of calling Fortran
b. the function &lt;code&gt;ctypes.c_pointer()&lt;/code&gt; can be used to create the necessary references to non-array elements, be sure to use &lt;code&gt;ctypes.c_int()&lt;/code&gt; and &lt;code&gt;ctypes.c_double()&lt;/code&gt; (and others) as required for the needed parameter
c. initializing arrays on the python end is strange. For example, say a Fortran subroutine is defined with an &lt;code&gt;integer, intent(out)&lt;/code&gt; array with size &lt;code&gt;nd&lt;/code&gt;, to initialize this array on the python side, you can use::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import ctypes as ct
intarray = (ct.c_int * nd)()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and to give the array values for input::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import ctypes as ct
pyarray = np.zeros(nd, dtype=np.int)
valarray = (ct.c_int * nd)(*pyarray)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Numpy provides some convenient methods &lt;code&gt;np.ctypeslib.as_ctypes()&lt;/code&gt; to pass initialized arrays with the correct types to the Fortran functions (as demonstrated above)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;mixed-language-debugging&#34;&gt;Mixed Language Debugging&lt;/h1&gt;

&lt;p&gt;The use of dll&amp;rsquo;s and &lt;code&gt;ctypes&lt;/code&gt; library for calling the Fortran code provides some exciting opportunities to speed up code development while leveraging the plotting and data management capabilities of pygeostat with the GSLIB Fortran geostatistical library. For example, to debug a Fortran dll which is present in a Visual Studio project, compiled with debug flags, generally follow these steps::&lt;/p&gt;

&lt;p&gt;Visual Studio 2015 Community is currently free and has recently developed a set of Python tools that will utilize Python distributions found on the machine. Combined with the Intel Compiler, projects containing Fortran code compiled to dll, and called VIA the Python libraries and tools presented above, can be debugged after it is called from Python!&lt;/p&gt;

&lt;h1 id=&#34;what-about-julia&#34;&gt;What about Julia&lt;/h1&gt;

&lt;p&gt;Julia provides first class support for calling C libraries using the &lt;code&gt;ccall&lt;/code&gt; function. The example above requires no modification on the fortran side:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module example
    use iso_c_binding
    implicit none
contains
    subroutine sqr_2d_arr(nd, val) BIND(C, NAME=&#39;sqr_2d_arr&#39;)
        !DEC$ ATTRIBUTES DLLEXPORT :: sqr_2d_arr
        integer, intent(in) :: nd
        integer, intent(inout) :: val(nd, nd)
        integer :: i, j
        do j = 1, nd
        do i = 1, nd
            val(i, j) = (val(i, j) + val(j, i)) ** 2
        enddo
        enddo
    end subroutine sqr_2d_arr
end module example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The module is compiled with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifort /dll example.f90
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gfortran -shared example.f90 -o example.dll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But now the call in Julia is vastly simplified:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;nd = Int(100)
arr = zeros(Int64, (nd, nd));
ccall((:sqr_2d_arr, &amp;quot;./example.dll&amp;quot;), Void, (Ref{Int64}, Ref{Matrix{Int64}}), nd, arr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;update-04-05-2018&#34;&gt;UPDATE 04-05-2018&lt;/h1&gt;

&lt;p&gt;Through some suffering I have learned of the joys of string interop between C and Fortran. I wrote some Fortran to facilitate the conversions. Maybe this will help someone somewhere someday&amp;hellip;&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/dektoud/a3b9bb0c485d2ae58143d71f55a8d9e4.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>AutoReloading PyCall imported Python modules in Julia</title>
      <link>https://dektoud.github.io/blog/post/julia-autoreloading/</link>
      <pubDate>Mon, 13 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://dektoud.github.io/blog/post/julia-autoreloading/</guid>
      <description>

&lt;h2 id=&#34;autoreloading-pycall-imported-modules&#34;&gt;AutoReloading PyCall Imported Modules&lt;/h2&gt;

&lt;p&gt;One of the major components of being productive with my Python workflows was the IPython magic command &lt;code&gt;%autoreload 2&lt;/code&gt;. This magic command throws a hook on the start of running an IPython cell, and recompiles Python modules where source code changes are detected. This is probably the single most useful component of my workflow since I modify and work on packages in Sublime and test and develop the code in an interactive Jupyter notebook. When I first started with PyCall in Julia, I sorely missed this feature. Julia itself offers a sort-of-autoreload for Julia modules using the package &lt;code&gt;ClobberingReload.jl&lt;/code&gt;. Although this (mostly) works for the Julia packages, any changed Python source code is omitted. To add this functionality to &lt;code&gt;PyCall.jl&lt;/code&gt;, I did some digging through IPython to figure out how Python does it and how it can be replicated for PyCall loaded modules in Julia.&lt;/p&gt;

&lt;p&gt;The first component is to generate a new class that mimics what is being done by the IPython Magic reloader:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from IPython.extensions.autoreload import ModuleReloader
import sys

class PyReloader:
    &amp;quot;&amp;quot;&amp;quot;
    Class pretty well taken directly from the IPython %autoreload magic function..
    very untested..
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self):
        self.mr = ModuleReloader()
        self.mr.enabled = True
        self.mr.check_all = True
        self.mr.check()
        self.loaded_modules = set(sys.modules)

    def reload(self):
        self.mr.check()
        self.reload_step2()

    def reload_step2(self):
        newly_loaded_modules = set(sys.modules) - self.loaded_modules
        for modname in newly_loaded_modules:
            _, pymtime = self.mr.filename_and_mtime(sys.modules[modname])
            if pymtime is not None:
                self.mr.modules_mtimes[modname] = pymtime
        self.loaded_modules.update(newly_loaded_modules)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, it should be loaded in Julia with PyCall, and made to run before each IJulia cell using:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using IJulia: push_preexecute_hook
using PyCall
@pyimport rmutils.jlutils.pyreimport as prl
pyreloader = prl.PyReloader()
push_preexecute_hook(() -&amp;gt; pyreloader[:reload]())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And now in an interactive Julia session, if Python code is loaded using PyCall, and modified, it will be automatically reloaded as if &lt;code&gt;%autoreload 2&lt;/code&gt; was working in the background!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Update - 25-11-2017&lt;/p&gt;

&lt;p&gt;Using the &lt;code&gt;ClobberingReload&lt;/code&gt; and the above PyReloader would casue things to get very messy when reloading Julia modules that contained the above python-reload code&amp;hellip; the following solves this issue:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# load the AutoReload hack for Python code in Julia
using IJulia: push_preexecute_hook, preexecute_hooks
using PyCall
@pyimport rmutils.jlutils.pyreimport as pyrl
pyreloader = pyrl.PyReloader()
pyloadfunc() = pyreloader[:reload]()
if !(pyloadfunc in preexecute_hooks)
    push_preexecute_hook(pyloadfunc)
    println(&amp;quot;loading pyautoreload...&amp;quot;)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The modified method first checks &lt;code&gt;preexecute_hooks&lt;/code&gt; to make sure the reload function hasnt already been added to the list of functions to execute. This ensures that the interactive Julia and Python sessions don&amp;rsquo;t duplicate the Python reload functions!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transitioning from Python to Julia</title>
      <link>https://dektoud.github.io/blog/post/julia/</link>
      <pubDate>Mon, 13 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://dektoud.github.io/blog/post/julia/</guid>
      <description>

&lt;h2 id=&#34;two-languages&#34;&gt;Two Languages&lt;/h2&gt;

&lt;p&gt;Python suffers from a two language problem. On one end the syntax, vast and mature library ecosystem, dynamic style and interactive workflows that Python offers provide a flexible and productive prototyping environment for new ideas. However, when moving past initial ideas and beginning to apply those ideas to datasets of meaningful size, &lt;em&gt;pythonistas&lt;/em&gt; often look to things like &lt;a href=&#34;https://dektoud.github.io/blog/blog/post/fast_subroutines/&#34;&gt;Cython, Numba, F2PY or C++&lt;/a&gt; to gain additional performance. In my research group we spend much of our time developing and implementing ideas using Python (or Matlab), and once those ideas are demonstrated, often a follow-up Fortran implementation is generated so that statically compiled executables implementing those ideas can reach as wide an audience as possible. Thus, to be successful we are required to work with some high-level and dynamic language and also some secondary language that can speed up the implementation and be compiled to a static executable for distribution. This two language problem is a common &lt;code&gt;pain point&lt;/code&gt; for numerical computing in Python.&lt;/p&gt;

&lt;h2 id=&#34;the-motivation&#34;&gt;The Motivation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://julialang.org/&#34;&gt;Julia&lt;/a&gt; claims to offer a fundamentally new paradigm targeting this problem. This language is tailored to numerical computing by offering a dynamic programming environment and static type definitions with JIT-compilation to within 1-2x the performance compiled languages (Fortran, C). Thus it is easy to see just why Julia is so appealing: fast development times with dynamic and interactive programming; seemingly painless code optimization using static typing and an LLVM compiler that gets speeds in the ballpark of compiled languages; and as a final perk to the language, a glimmer of hope that static compilation may one day be a standard part of the language.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;# dynamic containers
list = Any[]
# iteration
for item in items
    push!(list, item)
end
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;really-switching&#34;&gt;Really Switching?&lt;/h2&gt;

&lt;p&gt;One of the larger hurdles for actually switching from Python is the fact that I have developed a large suite of utilities that I depend on in my scripting and research workflows. One of those components is F2PY, which I use to write fast-running custom loops. I am hoping that Julia fills the requirements of fast-running code going forward.. but one important point of concern is losing all the utilities that I have worked so hard on in the past. Luckily a package &lt;code&gt;PyCall.jl&lt;/code&gt; allows Julia to interact directly with Python code. The syntax is a bit strange but a Python call like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pygeostat as gs
import numpy as np

x = np.random.randn(100)
y = np.random.randn(100)

ax = gs.scatxval(x, y)
# and to modify properties, e.g.:
ax.set_ylim([-4, 4])
ax.set_xlim([-4, 4])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;turns into this when called from Julia:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;using PyCall
@pyimport pygeostat as gs

x = randn(100)  # Julia includes this by default
y = randn(100)

ax = gs.scatxval(x, y)
ax[:set_ylim]([-4, 4])
ax[:set_xlim]([-4, 4])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note the similarity of the calls to the &lt;code&gt;scatxval()&lt;/code&gt; function. A large number of standard Python types are interoperable. One potential concern is the use of &lt;code&gt;pd.DataFrames()&lt;/code&gt; in Python versus the DataFrames in Julia.. but much of that concern can be alleviated by using Arrays which PyCall automates the type conversion back and forth between the Julia and Python objects. Infact, an entire Python-based workflow can be generated using existing scripts and tools developed in Python. Thus, any researcher who depends largely on Python can rest easy that their hard work will still aid them as they venture out to learn Julia.&lt;/p&gt;

&lt;h2 id=&#34;quick-speed-demo&#34;&gt;Quick Speed Demo&lt;/h2&gt;

&lt;p&gt;In my &lt;a href=&#34;https://dektoud.github.io/blog/blog/post/fast_subroutines/&#34;&gt;previous post&lt;/a&gt; I demonstrated a number of methods to improve the speed of a naive triple loop implementation of a gradient calculation for a 3-dimensional array. A similar implementation using the same triple-loop strategy in Julia is shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;quot; calculate the forward-reverse diffs on the 3D array&amp;quot;
function gridgradients(a)
    nx, ny, nz = size(a)
    gx = zeros(a)
    gy = zeros(a)
    gz = zeros(a)
    for k in 1:nz
        for j in 1:ny
            for i in 1:nx
                is = max(1, i - 1)
                ie = min(nx, i + 1)
                js = max(1, j - 1)
                je = min(nx, j + 1)
                ks = max(1, k - 1)
                ke = min(nx, k + 1)
                gx[i, j, k] = (a[ie, j, k] - a[is, j, k]) / 2.0
                gy[i, j, k] = (a[i, je, k] - a[i, js, k]) / 2.0
                gz[i, j, k] = (a[i, j, ke] - a[i, j, ks]) / 2.0
            end
        end
    end
    return gx, gy, gz
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Over 1000 iterations, this function averages $1.13ms$, which is ~1.5x the speed of Fortran or Cython, and almost 2x the speed of Numba with no additional decorations or work on my behalf. Note that the arrays are $1$ indexed, and the ordering of the fastest axis is familiar as it is the same as Fortran and the opposite of C and Python. The speedup here over pure Python is immense for code generated with similar effort.&lt;/p&gt;

&lt;p&gt;Recalling one of the optimizations that we chose in Cython:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@cython.boundscheck(False)
@cython.wraparound(False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A similar optimization is immediately available in the Julia code with the &lt;code&gt;@inbounds&lt;/code&gt; macro, which simply disables bounds-checks:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;gt;?@inbounds
@inbounds(blk)

Eliminates array bounds checking within expressions.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;&amp;quot; calculate the forward-reverse diffs on the 3D array&amp;quot;
function gridgradients_inbounds(a)
    nx, ny, nz = size(a)
    gx = zeros(a)
    gy = zeros(a)
    gz = zeros(a)
    @inbounds for k in 1:nz
        @inbounds for j in 1:ny
            @inbounds for i in 1:nx
                is = max(1, i - 1)
                ie = min(nx, i + 1)
                js = max(1, j - 1)
                je = min(nx, j + 1)
                ks = max(1, k - 1)
                ke = min(nx, k + 1)
                gx[i, j, k] = (a[ie, j, k] - a[is, j, k]) / 2.0
                gy[i, j, k] = (a[i, je, k] - a[i, js, k]) / 2.0
                gz[i, j, k] = (a[i, j, ke] - a[i, j, ks]) / 2.0
            end
        end
    end
    return gx, gy, gz
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this modified code runs in $0.67ms$ averaged over 1000 iterations. In other words, we are faster than both Cython and Fortran, and very close to Numba, and the syntax and complexity of the code hasn&amp;rsquo;t changed dramatically. The other touted benefit to the user (which I am still trying to wrap my head around as I learn more about Julia) is that all user defined types are statically compiled, so if &lt;code&gt;a&lt;/code&gt; was a matrix of:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;type MyFloat &amp;lt;: Real
    xi::Float64
    yi::Float64
end
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and certain methods are properly overloaded to make &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt; work correctly, the code using that custom type &lt;em&gt;would be as fast as&lt;/em&gt; that using the native types from the loops above.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;I am very new to Julia. The type system is familiar since Fortran had a form of multiple dispatch by defining function interfaces. Static typing is familiar, plotting is available using existing Python libraries. Static compiling to exe remains to be seen. Going forward I expect to carry out all work in Julia in hopes of finally dropping Fortran as my speed crutch.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
