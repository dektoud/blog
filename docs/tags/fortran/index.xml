<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fortran on Rmar&#39;s Guide</title>
    <link>https://dektoud.github.io/blog/tags/fortran/</link>
    <description>Recent content in Fortran on Rmar&#39;s Guide</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Ryan Martin</copyright>
    <lastBuildDate>Mon, 12 Mar 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="/blog/tags/fortran/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Calling Fortran DLL&#39;s from Python and Julia</title>
      <link>https://dektoud.github.io/blog/post/fortran_dlls/</link>
      <pubDate>Mon, 12 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://dektoud.github.io/blog/post/fortran_dlls/</guid>
      <description>

&lt;p&gt;The standard Python library contains a package called &lt;code&gt;ctypes&lt;/code&gt; which provides interfaces to foreign functions that are found in shared libraries. With the Intel compiler, a Fortran function or subroutine can be exported to dll by ensuring that the &lt;code&gt;!DEC$&lt;/code&gt; is present to define the exported symbol:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;! Note: this only works with the Intel Compiler
subroutine sqr(val)
    !DEC$ ATTRIBUTES DLLEXPORT, ALIAS:&#39;sqr&#39; :: sqr
    integer, intent(inout) :: val
    val = val ** 2
end subroutine sqr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the above subroutine is contained in the &lt;code&gt;example.f90&lt;/code&gt; file, it is compiled with :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifort /dll example.f90
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;which produces the shared library &lt;code&gt;example.dll&lt;/code&gt;. Inspecting this object with dumpbin:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ dumpbin /exports example.dll
1    0 00001000 sqr
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The function can be imported to Python with the &lt;code&gt;ctypes&lt;/code&gt; module and called with the following Python code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ctypes as ct

fortlib = ct.CDLL(&#39;example.dll&#39;)
val = 100
val = ct.pointer(ct.c_int(val)) # setup the pointer to the correct data structure
_ = fortlib.sqr(val)            # call the function
print(val[0])                   # index the memory address setup above
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is immediately clear that compiling the shared library in this way is easier (when compared to F2PY), and calling the Fortran from Python is much difficult. The advantages are that this is a C-independent method to build and link the Fortran extension, and the same dll called from Python here could also be called from other languages (e.g. Julia).&lt;/p&gt;

&lt;h1 id=&#34;maintaining-gnu-intel-compatibility&#34;&gt;Maintaining GNU - Intel compatibility&lt;/h1&gt;

&lt;p&gt;The method to call specific functions should be compiler independent. A method to maintain a compatible Python calling sequence for the dll&amp;rsquo;s is required since one set of Python code should be able to call the Fortran function regardless of how it was compiled. This can be done by using the &lt;code&gt;iso_c_binding&lt;/code&gt; module:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;subroutine sqr2(val) BIND(C, NAME=&#39;sqr2&#39;)
    use iso_c_binding
    !DEC$ ATTRIBUTES DLLEXPORT :: sqr2
    integer, intent(inout) :: val
    val = val ** 2
end subroutine sqr2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;iso_c_binding&lt;/code&gt; in this case overwrites the name mangling for each compiler (gfortran and ifort), and ensures the call sequence from Python is constant. In this case the &lt;code&gt;!DEC$&lt;/code&gt; statement is ignored by gfortran and the function is available in the exported dll. Notably, the &lt;code&gt;ALIAS&lt;/code&gt; is dropped from the &lt;code&gt;!DEC$&lt;/code&gt; export for Intel Fortran since the name in the binding overwrites this setting.&lt;/p&gt;

&lt;h1 id=&#34;more-complicated-data-types&#34;&gt;More Complicated Data Types&lt;/h1&gt;

&lt;p&gt;The main concern for calling Fortran functions in the dll&amp;rsquo;s from Python is how to setup the data structures. Consider the following function which take a 2-dimensional array of integers and computes some arbitrary value from the inputs by replacing the values in the array:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module example
    use iso_c_binding
    implicit none
contains
    subroutine sqr_2d_arr(nd, val) BIND(C, NAME=&#39;sqr_2d_arr&#39;)
        !DEC$ ATTRIBUTES DLLEXPORT :: sqr_2d_arr
        integer, intent(in) :: nd
        integer, intent(inout) :: val(nd, nd)
        integer :: i, j
        do j = 1, nd
        do i = 1, nd
            val(i, j) = (val(i, j) + val(j, i)) ** 2
        enddo
        enddo
    end subroutine sqr_2d_arr
end module example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The module is compiled with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifort /dll example.f90
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gfortran -shared example.f90 -o example.dll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and can be called from Python with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ctypes as ct
import numpy as np

# import the dll
fortlib = ct.CDLL(&#39;example.dll&#39;)

# setup the data
N = 10
nd = ct.pointer( ct.c_int(N) )          # setup the pointer
pyarr = np.arange(0, N, dtype=int) * 5  # setup the N-long
for i in range(1, N):                   # concatenate columns until it is N x N
    pyarr = np.c_[pyarr, np.arange(0, N, dtype=int) * 5]

# call the function by passing the ctypes pointer using the numpy function:
_ = fortlib.sqr_2d_arr(nd, np.ctypeslib.as_ctypes(pyarr))

print(pyarr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;important-considerations&#34;&gt;Important Considerations&lt;/h1&gt;

&lt;p&gt;In the above Python code some things are notable:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;All data is passed by pointer reference to the Fortran dll, the data is modified in place if the variable is &lt;code&gt;intent(inout)&lt;/code&gt;, or replaced if the variable is &lt;code&gt;intent(out)&lt;/code&gt;. All memory is controlled on the Python side of things&lt;/li&gt;
&lt;li&gt;The memory must be allocated in some form on the Python side using &lt;code&gt;np.zeros(size, dtype=type)&lt;/code&gt; (or similar) even if the variable is &lt;code&gt;intent(out)&lt;/code&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;The types of all data initialized on the Python size must match those being called in the Fortran module&lt;/strong&gt;
a. This is very important. For example, use &lt;code&gt;int&lt;/code&gt; for &lt;code&gt;integer&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt; for &lt;code&gt;real*8&lt;/code&gt;, etc. Python &lt;code&gt;float&lt;/code&gt; is double precision by default. This may not be a correct list, and is definitely not exhaustive, but it has worked with the limited testing that has been done with this method of calling Fortran
b. the function &lt;code&gt;ctypes.c_pointer()&lt;/code&gt; can be used to create the necessary references to non-array elements, be sure to use &lt;code&gt;ctypes.c_int()&lt;/code&gt; and &lt;code&gt;ctypes.c_double()&lt;/code&gt; (and others) as required for the needed parameter
c. initializing arrays on the python end is strange. For example, say a Fortran subroutine is defined with an &lt;code&gt;integer, intent(out)&lt;/code&gt; array with size &lt;code&gt;nd&lt;/code&gt;, to initialize this array on the python side, you can use::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import ctypes as ct
intarray = (ct.c_int * nd)()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and to give the array values for input::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import ctypes as ct
pyarray = np.zeros(nd, dtype=np.int)
valarray = (ct.c_int * nd)(*pyarray)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Numpy provides some convenient methods &lt;code&gt;np.ctypeslib.as_ctypes()&lt;/code&gt; to pass initialized arrays with the correct types to the Fortran functions (as demonstrated above)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;mixed-language-debugging&#34;&gt;Mixed Language Debugging&lt;/h1&gt;

&lt;p&gt;The use of dll&amp;rsquo;s and &lt;code&gt;ctypes&lt;/code&gt; library for calling the Fortran code provides some exciting opportunities to speed up code development while leveraging the plotting and data management capabilities of pygeostat with the GSLIB Fortran geostatistical library. For example, to debug a Fortran dll which is present in a Visual Studio project, compiled with debug flags, generally follow these steps::&lt;/p&gt;

&lt;p&gt;Visual Studio 2015 Community is currently free and has recently developed a set of Python tools that will utilize Python distributions found on the machine. Combined with the Intel Compiler, projects containing Fortran code compiled to dll, and called VIA the Python libraries and tools presented above, can be debugged after it is called from Python!&lt;/p&gt;

&lt;h1 id=&#34;what-about-julia&#34;&gt;What about Julia&lt;/h1&gt;

&lt;p&gt;Julia provides first class support for calling C libraries using the &lt;code&gt;ccall&lt;/code&gt; function. The example above requires no modification on the fortran side:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;module example
    use iso_c_binding
    implicit none
contains
    subroutine sqr_2d_arr(nd, val) BIND(C, NAME=&#39;sqr_2d_arr&#39;)
        !DEC$ ATTRIBUTES DLLEXPORT :: sqr_2d_arr
        integer, intent(in) :: nd
        integer, intent(inout) :: val(nd, nd)
        integer :: i, j
        do j = 1, nd
        do i = 1, nd
            val(i, j) = (val(i, j) + val(j, i)) ** 2
        enddo
        enddo
    end subroutine sqr_2d_arr
end module example
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The module is compiled with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ifort /dll example.f90
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gfortran -shared example.f90 -o example.dll
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But now the call in Julia is vastly simplified:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-julia&#34;&gt;nd = Int(100)
arr = zeros(Int64, (nd, nd));
ccall((:sqr_2d_arr, &amp;quot;./example.dll&amp;quot;), Void, (Ref{Int64}, Ref{Matrix{Int64}}), nd, arr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;update-04-05-2018&#34;&gt;UPDATE 04-05-2018&lt;/h1&gt;

&lt;p&gt;Through some suffering I have learned of the joys of string interop between C and Fortran. I wrote some Fortran to facilitate the conversions. Maybe this will help someone somewhere someday&amp;hellip;&lt;/p&gt;

&lt;script src=&#34;https://gist.github.com/dektoud/a3b9bb0c485d2ae58143d71f55a8d9e4.js&#34;&gt;&lt;/script&gt;
</description>
    </item>
    
    <item>
      <title>Methods to write fast subroutines for Python - minimal computer science required</title>
      <link>https://dektoud.github.io/blog/post/fast_subroutines/</link>
      <pubDate>Tue, 07 Feb 2017 21:00:01 -0700</pubDate>
      
      <guid>https://dektoud.github.io/blog/post/fast_subroutines/</guid>
      <description>

&lt;hr /&gt;

&lt;blockquote&gt;
&lt;p&gt;Note: I am not a Python or programming expert. There are probably better ways to optimize subroutines, but that is not the point of this post!.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Python is an impressive programming language for a beginning programmer/scientist. Syntax is easy to learn, the language is expressive (in the sense that it reads like it is doing things), and there are immense guides out there on the web to learn nearly any aspect of Python you can think of. I do not profess to be an expert of Python, by any means. I am self taught and know enough to make things happen for my research. I know (generally and definitely not enough) about the standard libraries, and I know enough about what &lt;em&gt;I want&lt;/em&gt; to happen in my code that I can find libraries meaningful for my research, i.e. numpy matrix manipulations, sparse linear algebra and sklearn machine learning algorithms.&lt;/p&gt;

&lt;p&gt;However, as with any research project, you are on the cutting edge and sometimes functions you require are not implemented in standard packages, or you require access to specific portions of the functions to insert new ideas. Our research group is historically invested in Fortran, so naturally, new code is written in Fortran to leverage the vast library of existing codes. Many libraries can be used within Fortran, however, its commonly easier to prototype and test outside of Fortran.&lt;/p&gt;

&lt;h1 id=&#34;methods-for-fast-computations-in-python&#34;&gt;Methods for &lt;em&gt;FAST&lt;/em&gt; computations in Python&lt;/h1&gt;

&lt;p&gt;Keeping in mind that we are at the cutting edge and need to develop fast subroutines for complicated numerical analyses, and also that our definition of &lt;strong&gt;&lt;em&gt;fast&lt;/em&gt;&lt;/strong&gt; is to execute numerical calculations in the least amount of time (as opposed to querying databases or other non-numerical tasks), what are the ways that we can do this within our chosen Python scripting universe? Anyone who has traveled down this road is likely familiar with the major options, and there are benefits and drawbacks to each of them. It depends on the application, complexity of the code, and the intended use of the code. I am focused on numerical computations in my research using linear algebra, matrix solvers, and machine learning techniques. Since my research group primarily produces the standalone Fortran executables, &lt;a href=&#34;https://docs.scipy.org/doc/numpy-dev/f2py/&#34;&gt;F2PY&lt;/a&gt; project is attractive for mixing Python and Fortran code. There are other methods to call Fortran code from Python, too, like &lt;a href=&#34;https://docs.python.org/3.5/library/ctypes.html&#34;&gt;ctypes&lt;/a&gt; or a library called &lt;a href=&#34;http://cffi.readthedocs.io/en/latest/&#34;&gt;cffi&lt;/a&gt;. However, all of these methods require fairly polished and working Fortran code that can be compiled and runs with minimal and easy to trace errors. This may not be completely suitable for speeding up research code since tracking down and fixing errors is difficult, especially for the newcomer. Compiling the code can be a headache in itself, especially if you are working on windows (which I am). As a beginning programmer and researcher learning about designing algorithms I guarantee that the flexibility in debugging and testing out subroutines that Python provides will improve the development of your ideas.&lt;/p&gt;

&lt;p&gt;A number of projects address the need for fast Python subroutines, like &lt;a href=&#34;http://www.numpy.org/&#34;&gt;Numpy&lt;/a&gt;, &lt;a href=&#34;http://pypy.org/&#34;&gt;PyPy&lt;/a&gt;,  &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;, &lt;a href=&#34;http://cython.org/&#34;&gt;Cython&lt;/a&gt;, and many many many others&amp;hellip;. I&amp;rsquo;m sure. So, for the beginning researcher? Which methodology provides the most benefit with the least amount of investment? I will focus on the following mainly because they are the ones that I have tried:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#F2PY&#34;&gt;F2PY&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#numpy&#34;&gt;Numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#numba&#34;&gt;Numba&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cython&#34;&gt;Cython&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each of the above methods attacks the problem in different ways. F2PY lets you compile Fortran code into easily callable python modules. Numpy provides a vast library of numerical computation functions that might allow you to skip the lower level languages all together. Numba uses &amp;lsquo;just in time&amp;rsquo; (JIT) compiling to &amp;lsquo;compile&amp;rsquo; native python code. This can be advantageous when the code consists of a number of loops with relatively simple mathematical operations. And finally Cython allows you to write static-typed &amp;lsquo;Python&amp;rsquo; that is compiled to C++ and taken to the extreme using Cython would allow you to write an entire project essentially in C++.&lt;/p&gt;

&lt;h1 id=&#34;example&#34;&gt;Example where fast code is needed&lt;/h1&gt;

&lt;p&gt;Lets consider an example of computing the gradients on a 3D grid using the forward-reverse difference at each cell. Assume that we don&amp;rsquo;t have any knowledge about specialized libraries that have already implemented this feature. Lets also assume that writing out the code in loop-format benefits our research so we can add in some ad-hoc calculations in the grid that &lt;em&gt;couldn&amp;rsquo;t&lt;/em&gt; be handled if we used an existing package to get the gradients, or if we used vectorized code (which Python is very good at).&lt;/p&gt;

&lt;h2 id=&#34;testing-the-code&#34;&gt;Testing the Code&lt;/h2&gt;

&lt;p&gt;I run Python 3.5 from Anaconda usually in the Jupyter notebook. A magic cell prefixed with &lt;code&gt;%time&lt;/code&gt; or &lt;code&gt;%%timeit&lt;/code&gt; &lt;em&gt;should&lt;/em&gt; be available to time functions, but these have never worked for me. Therefore I am using the following to test the functions implemented in this document:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time
def timefunc(function, args, n=100):
    stime = time.time()
    for _ in range(n):
        function(*args)
    return &#39;Time per iteration %.9f&#39; % ((time.time() - stime) / float(n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and tested with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def function ():
    return &#39;this is the function &#39;

print(timefunc(function, arguments))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The test data is constant between runs, and generated using numpy:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
grid = np.random.rand(50, 50, 50)
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;python-only&#34;&gt;Python Only&lt;/h1&gt;

&lt;p&gt;Lets start with un-vectorized Python code using loops (pretty much the worst case and something that should never be done in Python, but hey, we&amp;rsquo;re learning):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def calc_gradients(grid):
    nx, ny, nz = np.shape(grid)  # assumes the grid is 3D
    gx = np.zeros((nx, ny, nz))
    gy = np.zeros((nx, ny, nz))
    gz = np.zeros((nx, ny, nz))
    # the dreaded triple loop, a naive python implementation
    for i in range(nx):
        for j in range(ny):
            for k in range(nz):
                # limit the array indexes, remember about Python 0-indexing
                ist = max([0, i - 1])
                jst = max([0, j - 1])
                kst = max([0, k - 1])
                ifn = min([nx - 1, i + 1])
                jfn = min([ny - 1, j + 1])
                kfn = min([nz - 1, k + 1])
                # calculate the gradients
                gx[i, j, k] = ( grid[ist, j, k] - grid[ifn, j, k] ) / 2.0
                gy[i, j, k] = ( grid[i, jst, k] - grid[i, jfn, k] ) / 2.0
                gz[i, j, k] = ( grid[i, j, kst] - grid[i, j, kfn] ) / 2.0
    return gx, gy, gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which takes $0.365s$ per iteration. This isnt terribly bad. Some time is saved by using the standard library &lt;code&gt;max()&lt;/code&gt; function instead of the numpy &lt;code&gt;np.maximum()&lt;/code&gt; function.&lt;/p&gt;

&lt;h1 id=&#34;numba&#34;&gt;Numba&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt;, the JIT compiler, at a very high level takes Python code and makes machine code that runs very fast. Recent versions of Numba support many Numpy functions (you can &lt;a href=&#34;http://numba.pydata.org/&#34;&gt;check it out here&lt;/a&gt;). To my knowledge, this is the simplest way to speed up this type of triple loop happening in Python. A simple decorator &lt;code&gt;@numba.jit&lt;/code&gt; placed at the start of the function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numba
import numpy as np

@numba.jit
def calc_gradients_jit(grid):
    nx, ny, nz = np.shape(grid)  # assumes the grid is 3D
    gx = np.zeros((nx, ny, nz))
    gy = np.zeros((nx, ny, nz))
    gz = np.zeros((nx, ny, nz))
    # the dreaded triple loop, a naive python implementation
    for i in range(nx):
        for j in range(ny):
            for k in range(nz):
                # limit the array indexes, remember about Python 0-indexing
                ist = max(0, i - 1)
                jst = max(0, j - 1)
                kst = max(0, k - 1)
                ifn = min(nx - 1, i + 1)
                jfn = min(ny - 1, j + 1)
                kfn = min(nz - 1, k + 1)
                # calculate the gradients
                gx[i, j, k] = ( grid[ist, j, k] - grid[ifn, j, k] ) / 2.0
                gy[i, j, k] = ( grid[i, jst, k] - grid[i, jfn, k] ) / 2.0
                gz[i, j, k] = ( grid[i, j, kst] - grid[i, j, kfn] ) / 2.0
    return gx, gy, gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And the function runs in $0.695ms$, or about $525x$ faster average over 1000 iterations. Unfortunately if things outside of simple mathematical operations are required, and those are unsupported within the set of functions converted from Numpy (or other libraries) to standard Python implementations specifically for Numba, then you&amp;rsquo;re stuck implementing them on your own. So, this can&amp;rsquo;t be used for everything. But in this simple case, it is the best!&lt;/p&gt;

&lt;h1 id=&#34;cython&#34;&gt;Cython&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://cython.org/&#34;&gt;Cython&lt;/a&gt; is a nice library that essentially lets you write compiled C extensions with familiar Python-like syntax. Speed comes from declaring static types with specialized code. Cython is essentially its own syntax although many things are similar to Python or C (although I have never used the latter). The Cython code is converted to C using all sorts of trickery that isn&amp;rsquo;t important at this level of understanding, and the result is an extension module that can be imported and called from Python, and has all the speed of a compiled language. I think Cython can be quite flexible, but the most speed comes from being explicit and defining the types of all variables that are used in the code. Special syntax is required for this part which can be a bit confusing to learn, but there are lots of examples out there. So, our 3D gradient subroutine becomes (using the Cython magic cell):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%%cython
import numpy as np
cimport numpy as np
cimport cython

@cython.boundscheck(False) # these flags may make problems..
@cython.wraparound(False)
def calc_gradients_cy(np.ndarray[np.double_t, ndim=3] grid):
    # explicitly define ***ALL*** variables types
    cdef int nx = grid.shape[0]
    cdef int ny = grid.shape[1]
    cdef int nz = grid.shape[2]
    cdef int i, j, k, ist, jst, kst, ifn, jfn, kfn
    cdef np.ndarray[np.double_t, ndim=3] gx = np.zeros((nx, ny, nz), dtype=np.double)
    cdef np.ndarray[np.double_t, ndim=3] gy = np.zeros((nx, ny, nz), dtype=np.double)
    cdef np.ndarray[np.double_t, ndim=3] gz = np.zeros((nx, ny, nz), dtype=np.double)

    # the dreaded triple loop, a naive python implementation
    for i in range(nx):
        for j in range(ny):
            for k in range(nz):
                # limit the array indexes, remember about Python 0-indexing
                ist = max(0, i - 1)
                jst = max(0, j - 1)
                kst = max(0, k - 1)
                ifn = min(nx - 1, i + 1)
                jfn = min(ny - 1, j + 1)
                kfn = min(nz - 1, k + 1)
                # calculate the gradients
                gx[i, j, k] = ( grid[ist, j, k] - grid[ifn, j, k] ) / 2.0
                gy[i, j, k] = ( grid[i, jst, k] - grid[i, jfn, k] ) / 2.0
                gz[i, j, k] = ( grid[i, j, kst] - grid[i, j, kfn] ) / 2.0
    return gx, gy, gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function comes in slightly behind Numba at $0.953ms$ average over 1000 iterations, which is a $383x$ speedup. However, this comes at a cost of much more complex conversion of the original code to statically type everything. Additionally, there are likely Cython best practice conventions that I have not considered, and therefore the above code may be suboptimal.&lt;/p&gt;

&lt;h1 id=&#34;F2PY&#34;&gt;F2PY&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.scipy.org/doc/numpy-dev/f2py/&#34;&gt;F2PY&lt;/a&gt; is my favorite methodology to compile fast code, but this is because: 1) my research group uses Fortran, 2) I know Fortran, and 3) I have mastered F2PY on Windows with the Intel Compiler. This method is definitely not where I would start if I am learning. Many people view Fortran as an ancient language not worth learning. I agree that prototyping code and ideas in Fortran can be arduous, but the open source GSLIB programs released in the 90&amp;rsquo;s, which are still relevant, pretty well sum up how useful Fortran can be. Distributing static executables allows you to reach a wider audience than Python which is notoriously hard to distribute on Windows to people who don&amp;rsquo;t use Python.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-fortran&#34;&gt;subroutine calc_gradients(nx, ny, nz, grid, gx, gy, gz)
  implicit none
  integer, intent(in) :: nx, ny, nz
  real*8, intent(in) :: grid(nx, ny, nz)
  real*8, intent(out) :: gx(nx, ny, nz), gy(nx, ny, nz), gz(nx, ny, nz)
  integer :: i, j, k, ist, jst, kst, ifn, jfn, kfn
  ! triple loop, reverse fastest axes
  do k = 1, nz
    do j = 1, ny
      do i = 1, nx
        ! ensure the indices do not go out of bounds
        ist = max(1, i - 1)
        jst = max(1, j - 1)
        kst = max(1, k - 1)
        ifn = min(nx, i + 1)
        jfn = min(ny, j + 1)
        kfn = min(nz, k + 1)
        ! calculate each gradient
        gx(i, j, k) = ( grid(ist, j, k) - grid(ifn, j, k) ) / 2.0D0
        gy(i, j, k) = ( grid(i, jst, k) - grid(i, jfn, k) ) / 2.0D0
        gz(i, j, k) = ( grid(i, j, kst) - grid(i, j, kfn) ) / 2.0D0
      enddo
    enddo
  enddo
end subroutine calc_gradients
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function runs in roughly the same amount of time as Cython at $0.993ms$ average over 1000 iterations, which is a $367x$ speedup. However, this is a totally different language and if you need to change the code and recompile the function with F2PY, you have to restart the kernel in order to reimport the modified Fortran extension, which ends up loosing you some efficiency in the long run. This method is extremely useful if your end use case is to deploy with Fortran executables, of if you want to leverage some ancient Fortran library that isn&amp;rsquo;t already wrapped for Python. However, it&amp;rsquo;s hard to recommend this method for newcomers since the other methods are simpler to get fast code going.&lt;/p&gt;

&lt;p&gt;The common issue with linking Fortran to Python (mainly for Windows users) is with F2PY. Depending on the version of Python, Fortran compiler, and platform, this can either be a breeze, or a complete pain. If you are having some issues in Windows, you can check out the &lt;a href=&#34;http://www.ccgalberta.com/pygeostat/fortran.html&#34;&gt;pygeostat documentation on Fortran in Python&lt;/a&gt; for some examples on common errors and some suggested solutions. Typically the fact of running Windows is the single greatest hurdle to using F2PY for this workflow. Luckily &lt;a href=&#34;http://www.ccgalberta.com/pygeostat&#34;&gt;pygeostat&lt;/a&gt; has taken care of the hard work and implements a custom F2PY compiling script for Windows that works across several versions of Python and with Intel and GNU Fortran compilers.&lt;/p&gt;

&lt;h1 id=&#34;numpy&#34;&gt;So, what about straight up Numpy?&lt;/h1&gt;

&lt;p&gt;Of course, for a task such as computing the gradients on the grid (this is a very common task), Numpy and its vast library of computational functions should probably be investigated. A quick Google search for &lt;code&gt;&#39;numpy gradients&#39;&lt;/code&gt; returns the &lt;a href=&#34;https://docs.scipy.org/doc/numpy/reference/generated/numpy.gradient.html&#34;&gt;numpy.gradient&lt;/a&gt; function which can be used to quickly compute the gradients of the 3D grid. The implementation is simple:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from numpy import gradient

gradient(grid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The numpy gradient function runs only a little bit slower than Cython and Fortran at $2.50ms$ averaged over 1000 iterations, which is a $146x$ speedup. Unfortunately, since we offload the numerical component of the calculation, we loose access to the portion of the subroutine that we wanted, to inject some custom code at each cell.&lt;/p&gt;

&lt;h1 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;There are a variety of methods to make subroutines run faster in Python. The best way to prototype new ideas is definitely to &lt;em&gt;NOT&lt;/em&gt; write research code in Fortran, rather use some mixed Python-Speed methodology as outlined above to develop ideas quickly while writing readable and quickly modifiable code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
